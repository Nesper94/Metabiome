# 2020-09-02

- El script de HUMAnN2 no funcionaba porque aparecía un error relacionado con
MetaPhlAn2. El error parece haberse solucionado al instalar la versión 2.8 de
MetaPhlAn2 en el ambiente humann2 con
`conda install -c bioconda metaphlan2=2.8`

- Para correr HUMAnN2 primero debe descargarse la base de datos de Chocophlan
con el comando
```
humann2_databases --download chocophlan full <directory>
```
y la base de datos de Uniref con el comando
```
humann2_databases --download uniref uniref90_diamond $INSTALL_LOCATION
```
La pregunta es: ¿Debería hacer esto el usuario o debería hacerlo nuestro
script?  
R/ Lo debería hacer el usuario pero debe indicarse en nuestro script.

- Crear archivos de ambientes de Conda para que los usuarios puedan instalar
fácilmente todo el software necesario. Estos ambientes se almacenarán en la
carpeta `conda-envs/`.

- Se habló de escribir documentación en Sphinx (para el futuro, cuando otra
    gente lo necesite).

## Por hacer

- [x] **Juan:** Seguir con MEGAHIT.
- [x] **Estefa:** Qiime2 y MetaSPADES.
- [x] **Cristian:** Kaiju.
- [x] El viernes hacer las diapositivas de la presentación del pipeline.
- [ ] Ir escribiendo la documentación del pipeline.
- [x] Mostrar el plugin [SHOGUN](https://library.qiime2.org/plugins/q2-shogun/15/)
de Qiime2.

# 2020-09-04 10:30 UTC-5

Hicimos las diapositivas de la presentación. Quedamos en que Estefa iba a hacer la presentación.

## Por hacer
- [x] Mencionar en la presentación el PMA, Conda, etc.
- [x] Mencionar el artículo que estamos usando como material de prueba.
- [x] Reunirnos el domingo a las 2 pm.
- [x] Mejorar el diagrama del pipeline.

# 2020-09-09 11:00 UTC-5

Se habló de:

- Para exportar los ambientes de Conda es mejor usar el comando `conda env export --from-history` como se explica [aquí](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#exporting-an-environment-file-across-platforms).
- Mostrar `guidelines.md`
- Listo script de MEGAHIT.
- Pedir más información y literatura a expertos en el asunto (e.g. [C. Titus Brown](http://ivory.idyll.org/blog/)).
- Mostrar [artículo workflows](https://dib-lab.github.io/2020-workflows-paper/).

Cristian nos recomendó leer un [artículo](https://www.pnas.org/content/113/49/E7996) donde hacen gráficas y análisis
interesantes que nos pueden servir en nuestro trabajo. Este artículo y el de workflows se discutirán el próximo miércoles.

## Por hacer

- Asegurarse que los errores de nuestros scripts sean enviados al STDERR (>&2).
- **Estefa:** Revisar Qiime2 para ver si sí se puede usar como input los resultados de BBDuk.
- **Cristian:** Script de Kaiju.
- **Juan:**
    - [x] Escribir correos (Cristian escribirá a los de metaWRAP).
    - [x] Incluir MetaPhlAn3 en el diagrama de flujo (Taxonomic binning).
    - [x] Poner visualizaciones de outputs como opcionales en el flujograma.

# 2020-09-11 11:00 UTC-5

Ya está listo el script de Kaiju. Estefa nos comentó que sí es posible
importar el output de BBDuk a Qiime2.

## Por hacer

- [x] Leer los artículos para el miércoles.
- [x] Leer el informe de Cristian.
- [ ] Empezar a escribir documentación.
- [ ] Asegurarse que los errores de nuestros scripts sean enviados al STDERR (>&2).
- Juan:
    - [x] Poner actas en GitHub.
    - [ ] Arreglar script Trimmomatic.
    - [ ] Evaluación del ensamblaje con MetaQUAST.
    - [ ] Revisar si se puede hacer algún tipo de híbrido con metaWRAP.

# 2020-09-16 11:00 UTC-5

Discutimos lo de la profundidad de secuenciamiento. Estefa nos mostró [esta
página](https://genohub.com/ngs/) en donde se pueden obtener resultados de
diferentes experimentos de secuenciamiento. Quedamos en que 12Gbp por muestra
es un buen número.

- Mostrar [este artículo](https://www.biorxiv.org/content/10.1101/737528v1).
- Y también [este otro](https://github.com/sunbeam-labs/sunbeam).
- Enseñar LaTeX.
- Enseñar Markdown.
- Empezar a mirar la documentación con Sphinx.
- Mostrar [Zenodo](https://zenodo.org/).
- Mostrar los [Principios FAIR](https://www.nature.com/articles/sdata201618).

## Por hacer:

- [ ] Poner tiempo límite de ejecución en los archivos de SLURM del cluster:  
`#SBATCH --time=11-00:00:00`
- [ ] Leer [este artículo](https://doi.org/10.1371/journal.pcbi.1005510).
- [x] Mañana a las 20:00 acordar un esquema de nomenclatura para los archivos.
- [ ] Discutir cómo activar Conda desde un script.

# 2020-09-17 20:00 UTC-5
## Reunión nomenclatura de archivos

El siguiente es un ejemplo del nombre de un archivo output de Trimmomatic y que
usa un esquema de nomenclatura anterior a esta reunión:  
`BB190_L001_f-paired.fq.gz`

Decidimos nombrar los archivos de los forward con "1" y los reverse con "2".
También decidimos usar **snake_case**.

Según esta nueva convención, por ejemplo, los archivos output de Trimmomatic 
tendrían la siguiente estructura:

```
BB190_L001_1_paired_trim.fq.gz
BB190_L001_1-unpaired_trim.fq.gz
BB190_L001_2-paired_trim.fq.gz
BB190_L001_2-unpaired_trim.fq.gz
```

Después de ser procesados por Bowtie2 tendrían la siguiente estructura:  
`BB190_L001_1_paired_bt2.fq.gz`

Para cambiar el nombre a un archivo después de haber pasado por
Bowtie2 en el script podemos usar este comando: `sed 's/trim/bt2/'`

## Por hacer

- [ ] Cambiar los scripts de manera que los nombres sigan la convención acordada.
- [ ] Escribir en la documentación la convención de nomenclatura de los archivos.
- [ ] Escribir también las abreviaciones de los pasos del pipeline (e.g. 
Bowtie2=bt2).
- [ ] Dejar `paired` y `unpaired` en los nombres de archivos, y especificar en la
documentación que se trata del output de Trimmomatic (los reads que quedaron con
pareja y los que no).
- [ ] Leer [artículo de María](http://www.musalit.org/seeMore.php?id=19153).

























